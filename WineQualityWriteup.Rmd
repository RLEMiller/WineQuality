---
title: "Wine Quality Prediction from Physiochemical Properties"
author: "Rebecca L.E. Miller"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    keep_tex: yes
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE, echo=FALSE}
if(!require(tinytex)) install.packages("tinytex", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "https://cran.us.r-project.org")
if(!require(GGally)) install.packages("GGally", repos = "http://cran.us.r-project.org")
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org")
if(!require(gam)) install.packages("gam", repos = "http://cran.us.r-project.org")

library(knitr)
library(tinytex)
library(tidyverse)
library(caret)
library(data.table)
library(GGally)
library(corrplot)
library(gam)

# don't show messages for tidy summarize() calls
options(dplyr.summarise.inform = FALSE) 

# set the defaults for chunk options
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = FALSE)
```

# Summary

# Introduction

The data used in this analysis is from the University of California Irvine Machine Learning Repository, data set number 1341136, or the Wine Quality data set from "Modeling Wine Preferences by Data Mining from Physiochemical Properties." \footnote{P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.} The data consists of eleven physiochemical properties and a quality score between 0 and 10 for each wine in the set. There are two subsets, both for Portuguese "vinho verde" wines, one for red wine and one for white wine. The most obvious machine learning task for this dataset is to use the physiochemical data as predictors and the quality score as the response variable, or in more direct language, to use the physiochemical properties to predict quality. More succinctly

Task: Predict good, bad and average wine based on physiochemical properties.

## Download Data
The data set is available in two *.csv files found at [red](https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv)
and [white](https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv). The files can be downloaded with the following code.

```{r DownloadFiles, echo=TRUE, message=FALSE}
  ###-----------------------------------------------------------
  ###     Download And Parse Files From Remote Repository
  ###----------------------------------------------------------
  # create a temporary file in the local filesystem for download of raw csv
  dl <- tempfile()
  
  # download the file
  download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv", dl)
  
  # read the red wine quality file into a data frame
  red <- read.csv(file = dl, header = TRUE, sep = ";")
  
  # remove the dl file handle
  rm(dl)
  
  #--------------------- White Wine File ---------------------
  # make a new temp file
  dl1 <- tempfile()
  
  # Download raw csv
  download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv", dl1)
  
  # read the white wine quality file into a data frame
  white <- read.csv(file = dl1, header = TRUE, sep = ";")
  
  rm(dl1)
```

## Data Exploration
The physiochemical properties, or columns of the data sets, for the wines in the two files are
```{r echo=TRUE, message=FALSE }
  union(colnames(red), colnames(white))
```

Both files have the same physiochemical property columns
```{r echo=TRUE, message=FALSE}
  setdiff(colnames(red),colnames(white))
```

Neither data set contains `NA` values.
```{r}
  sum(is.na(red))
  sum(is.na(white))
```
Now that we have a basic understanding of the structures of the two data sets, we can combine them into a single data set. We can try to predict wine quality based on the physiochemical properties and the wine color category that we create when we join the two data sets.  We also will designate a hold-out subset and a test subset that we will use to develop the model. 

```{r catRedWhite, message=FALSE, warning=FALSE}
  # make new columns for the wine color or type
  red <- red %>% mutate(red = 1)
  white <- white %>% mutate(red = 0)
  
  # combine the two data frames into one
  wines <- red %>% full_join(., white)
  
  # set the seed for reproducibility, user set.seed(1234) for R 3.5 or earlier
  set.seed(1234, sample.kind="Rounding") 
  
  # use createDataPartition as in the Data Science course material
  # set aside 10% of the data as a final test set, which we call hold_out
  test_index <- createDataPartition(y = wines$quality, times = 1, 
                                    p = 0.1, list = FALSE)
  full_train <- wines[-test_index, ]
  hold_out <- wines[test_index, ]
  
  # partition the data a second time to create test and train sets from the 
  # full_train set
  set.seed(1234, sample.kind="Rounding") 
  test_index2 <- createDataPartition(y = full_train$quality, times = 1, 
                                    p = 0.1, list = FALSE)
  train <- full_train[-test_index2, ]
  test <- full_train[test_index2, ]
                        
```

```{r correlationPlot, fig.cap="Plot of intercorrelation in the train set."}
  # Make a correlation plot of the train set
  corrplot(cor(train), order = "AOE", tl.col = "black")
```
We can also look at histograms of our variables in the `train` set to get some idea of the character of the data.

```{r histograms, echo = FALSE, message=FALSE, warning=FALSE, fig.cap="Histograms of all the variables in the train set with the exception of the type variable that we created to keep track of white/red status."}
  train %>%
    select(-red) %>%
    gather() %>%
    ggplot(aes(value)) +
      facet_wrap(~key, scales = "free", ncol = 3) +
      geom_histogram()
```

We see that quality is approximately normally distributed, with few values in the set of {4,8} and most values in the set {5,6,7}. That is the expected result, with just a few very good wines and a few very bad ones.

Other approximately normally distributed predictors include `chlorides`, `citric.acid`, `fixed.acidity`, `pH`, `sulphates`, and `volatile.acidity`. The remaining predictors are either bimodal, like `total.sulfur.dioxide` or are significantly skewed, as with `alcohol` content.

With a basic grasp of the shape and size of our data, we can begin to construct models. Because `quality` is a continuous set of natural numbers, we can allow our prediction to be continuous, as well. Our predictions will be the continuous set for the range of the training set,
```{r}
range(train$quality)
```
We can therefore choose our loss function as something appropriate for continuous data. We will use Mean Absolute Error
$$
 MAE = \frac{1}{N}\sum_{i=1}^{N}\vert\hat{y}_{i} - y_i\vert
$$
which is a simple way to measure the magnitude of the prediction error and has the advantage, like RMSE, of being on the same scale as the data. We will use the following function to calculate $MAE$.

```{r MAE, echo=TRUE}
  MAE <- function(predicted, true){
    error <- mean(abs(predicted - true))
    return(error)
  }
```

# Methods

## The Single Value Model
If we predict a single value for all of the wine quality values, the minimum error will be the mean of the `quality` score for the entire `train` set.

```{r mu, echo=TRUE}
  mu <- mean(train$quality)
```
Then the error is
```{r firstErrorMu, echo=TRUE}
  MAE(test$quality, mu)
```

```{r initializeResults}
  results <- tibble(method = "Just the average", 
                    MAE = MAE(test$quality, mu))

  knitr::kable(results)
```

## Linear Model
We missed the prediction by just over $\frac{2}{3}$ of a `quality` score, on average, but of course the largest misses are at the top and bottom of the scale. In other words, we failed to predict the best and worst wines. We defined our goal in terms of the task of predicting good, bad and mediocre wines, and the single value does a poor job of this. 

If a single value does not perform well, we can next attempt to compute a linear combination of the available predictors, including wine type. We can use the `caret` package to create this initial model very quickly and easily.

```{r LinearModel}
  fit <- train(quality~., data = train, method="lm")
  y_hat <- predict(fit, test)
  MAE(test$quality, y_hat)
```

```{r LinearModelResults}
  results <- results %>% 
    add_row(method = "Basic Linear Model, All Predictors", 
            MAE = MAE(test$quality, y_hat))
```

This is not a bad result, and does offer an improvement over a single value prediction, but if we take a look at the prediction, `y_hat`, versus the values in `test$quality`, we see that the linear predictor doesn't perform well at the ends of the `quality` scale, and those values are the ones we would most like to predict correctly. Additionally, if we read the documentation for linear prediction with `caret`, we find that there are multiple pre-processing options. We can center, scale, replace predictors with the Principal Components via Principal Component Analysis (PCA), or replace with independent components. We can examine how pre-processing might improve our results.

```{r fig.height=3, fig.cap="Plot of the linear model prediction for wine quality in the test set against the actual wine quality value."}
  qplot(test$quality, y_hat)
```

```{r preProcessedLinearModel}
  # Re-do linear model with preprocessing
  # first create the parameters for pre-processing by 
    # subtracting the column means, dividing by standard deviation, and 
    # computing the principal components
  preProc <- train %>% 
    select(-quality) %>% 
    caret::preProcess( . , method = c("center", "scale", "pca"))

  # Create transformed test and train sets
  train_transformed <- predict(preProc, train)
  test_transformed <- predict(preProc, test)
  
  # Re-train the linear model on the transformed data
  fit <- train(quality~., data = train_transformed, method="lm")
  y_hat <- predict(fit, test_transformed)
  MAE(test_transformed$quality, y_hat)
  
```

With the transformation of our data via centering, scaling, and PCA we did improve the performance of the linear model. However, we see that the performance was still best in the mid-range of actual quality scores and poorest in the extremes of the range.

```{r PreprocLinearResults, echo=FALSE}
  results <- results %>% 
    add_row(method = "Basic Linear Model, Centered, Scaled, PCA", 
            MAE = MAE(test_transformed$quality, y_hat))
```

```{r LinearModelMAEvsQuality, fig.cap="Linear model of transformed test and train sets, MAE vs factorized quality. Shows best performance in the middle range and poor performance with both low and high actual quality scores.}
  test_transformed %>% select(quality) %>%
    mutate(factored_quality = as_factor(quality), y_hat = y_hat) %>%
    group_by(factored_quality) %>% 
    summarize(MAE = mean(abs(quality - y_hat))) %>%
    ggplot(aes(factored_quality, MAE)) +
      geom_point()

```

## K Nearest Neighbors Model

We can try another model to see if we can improve our results in the extremes of the quality scale. We can approximate a higher-degree linear model with k-Nearest Neighbors. We can also examine if the pre-processing improves the effectiveness of kNN.

```{r knnModel}
  control <- trainControl(method = "cv", number = 10, p = 0.9)
  fit <- train(quality ~ ., 
               method = "knn",
               data = train,
               tuneGrid = data.frame(k = seq(2,40,2)),
               trControl = control)
  ggplot(fit, metric = "MAE", highlight = TRUE)
  y_hat <- predict(fit, test)
  MAE(test$quality, y_hat)
  
  fit_transformed <- train(quality ~ .,
                           method = "knn",
                           data = train_transformed,
                           tuneGrid = data.frame(k = seq(2,40,2)),
                           trControl = control)
  ggplot(fit_transformed, highlight = TRUE)
  y_hat_transf <- predict(fit_transformed, test_transformed)
  MAE(test_transformed$quality, y_hat_transf)
```

```{r knnModelResults}
  results <- results %>% 
    add_row(method = "KNN Model, All Predictors", 
            MAE = MAE(test$quality, y_hat))
```

## Random Forest

```{r RandomForestModel}
  fit <- train(quality ~ ., 
               method = "rf",
               ntree = 50,
               data = train)
  y_hat <- predict(fit, test)
  MAE(test$quality, y_hat)
```



## General Additive Model

The 

```{r gamModel}
  # Create a function to clip inputs so that the range fits within the given
  # upper bounds (UB) and lower bounds (LB)
  SimpleClip <- function(b,UB,LB){
    b[b > UB] <- UB
    b[b < LB] <- LB
    return(b)
  }

  grid <- expand.grid(span = seq(0.15, 0.65, len = 10), degree = 1)
  fit <- train(quality ~ ., 
               method = "gamLoess",
               tuneGrid = grid,
               data = train)
  y_hat <- SimpleClip(predict(fit, test), UB=10,LB=1)
  MAE(test$quality, y_hat)
```

```{r GenAdditiveModelResults}
  results <- results %>% 
    add_row(method = "Generalized Additive Model, All Predictors", 
            MAE = MAE(test$quality, y_hat))
```



If we assign the factors "good", "mediocre", and "bad" based on the Tukey method for determining outliers, we get

```{r TukeyOutliers}
  # Third quartile, or 75% of data is less than this value, less 1.5 inter-quartile range
  max_avg_quality <- quantile(train$quality, 0.75) + 1.5 * IQR(train$quality)
  # First quartile, or 25% of data is less than this value, minus 1.5 inter-quartile range
  min_avg_quality <- quantile(train$quality, 0.25) - 1.5 * IQR(train$quality)
  
  train <- train %>%
    mutate(good = case_when(
      quality > max_avg_quality ~ 1,
      quality < min_avg_quality ~ 0,
      TRUE ~ -1)
    )
    
```

# Results

# Conclusion